{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnigmaNet: A Neural Network Framework for Eplipesy Classification\n",
    "### This is a script that uses Keras to predict epilepsy outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Details\n",
    "This script is designed to run with the following configuration:\n",
    "* Keras\n",
    "* TensorFlow backend\n",
    "* PlaidML backend (if using Radeon GPU)\n",
    "* Tabular data (Excel or CSVs)\n",
    "\n",
    "## Dependencies\n",
    "Run the following command to pip install all dependencies\n",
    "```\n",
    "pip install numpy pandas sklearn matplotlib scipy keras_tqdm\n",
    "```\n",
    "\n",
    "Also install NeuroCombat at https://github.com/ncullen93/neuroCombat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to Data\n",
    "Define path to tabular data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = '/Users/sid/Documents/Projects/Enigma-ML/Dataset/Diffusion/MD.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Script Parameters\n",
    "Define parameters that control script flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classSel = 'Dx'             # Class labels\n",
    "dBegin = 'ACR'              # Column where data begins\n",
    "dEnd = 'UNC.R'   # Column where data ends\n",
    "cBegin = 'Site'             # Column where covariates/demographics begin\n",
    "cEnd = 'Sex'                # Column where covariates/demographics end\n",
    "fillmissing = True          # Fill missing?\n",
    "harmonize = False            # Run ComBat harmonization?\n",
    "scaleData = True            # Rescale data?\n",
    "dataSplit = 0.10            # Percent of data to remove for validation\n",
    "nEpochs = 250               # Training number of epochs\n",
    "bSize = 30                  # Training batch size\n",
    "plotType = 'Normal'         # Type of ComBat graphs to save ('Histogram' or 'Normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ComBat Harmonization Parameters\n",
    "Python implementation of ComBat is currently being used, which can be found at https://github.com/ncullen93/neuroCombat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combat Variables\n",
    "if harmonize:\n",
    "    batchVar = 'Site'           # Batch effect variable\n",
    "    discreteVar = ['Dx','Sex']  # Variables which are categorical that you want to predict\n",
    "    continuousVar = ['Age']     # Variables which are continuous that you want to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules\n",
    "Import all modules required to process the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuroCombat import neuroCombat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent Function\n",
    "Functions that the scrips will depend on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classfill(dFrame, classSel, idxRange):\n",
    "    \"\"\"Fills missing values with means of a class\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    dFrame:   Pandas dataframe to process (type: dataframe)\n",
    "            \n",
    "    classSel: String indicating dataframe column name containing class information\n",
    "    \n",
    "    idxRange: 2x1 vector indicating lower and upper bound of data to fill in dataframe\n",
    "              idxRange[0] is lower bound\n",
    "              idxRange[1] is upper bound\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data:     Dataframe will all missing values filled\n",
    "    \"\"\"\n",
    "    uniqClass = dFrame[classSel].unique()           # All unique classes\n",
    "    print('...found ' + str(uniqClass.size) + ' classes')\n",
    "    print('...filling missing data with class means')\n",
    "    data = dFrame.loc[:, idxRange[0]:idxRange[1]]                 # Extract all numerical value from 'dBegin' onwards\n",
    "    for c in uniqClass:\n",
    "        classIdx = dFrame.loc[:, classSel] == c     # Index where class is uniqClass = c\n",
    "        for n in range(len(data.columns)):\n",
    "            nanIdx = data.iloc[:,n].isnull()           # Index missing values\n",
    "            # Compute mean of class values without nans\n",
    "            # Because a Series of booleans cannot be used to index a dataframe, use the values attribute\n",
    "            # to extract a bool array\n",
    "            mu = np.nanmean(data.iloc[classIdx.values, n])\n",
    "            data.iloc[nanIdx.values, n] = mu\n",
    "    dFrame.loc[:,idxRange[0]:idxRange[1]] = data\n",
    "    return dFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files and Manipulate\n",
    "Load tabular data and prime it for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...found 2 classes\n",
      "...filling missing data with class means\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c4b6a281b230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Initialize scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscaleData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cData' is not defined"
     ]
    }
   ],
   "source": [
    "dFrame = pd.read_csv(csvPath)           # Dataframe\n",
    "if fillmissing:\n",
    "    dFrame = classfill(dFrame, classSel, [dBegin, dEnd])\n",
    "else:\n",
    "    print('...skip fill missing')\n",
    "\n",
    "# Run combat\n",
    "if harmonize:\n",
    "    cData = neuroCombat(data=dFrame.loc[:,dBegin:dEnd],\n",
    "                          covars=dFrame.loc[:,cBegin:cEnd],\n",
    "                          batch_col=batchVar,\n",
    "                          discrete_cols=discreteVar,\n",
    "                          continuous_cols=continuousVar)\n",
    "\n",
    "data = np.array(dFrame.loc[:, dBegin:dEnd])     # Preserve non-harmonized data\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()   # Initialize scaler\n",
    "if scaleData:\n",
    "    if harmonize:\n",
    "        cData = scaler.fit_transform(cData)\n",
    "    data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data for Training and Evaluation\n",
    "Data needs to be split for trianing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets and scale\n",
    "if harmonize:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cData, dFrame.loc[:, classSel], test_size=dataSplit, random_state=0)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, dFrame.loc[:, classSel], test_size=dataSplit, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Train\n",
    "A perceptron is constructed here for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose whether data/labels or X_Train,y_train\n",
    "dataIn = X_train\n",
    "labelsIn = y_train\n",
    "\n",
    "# Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the Single Perceptron or Shallow network\n",
    "model.add(Dense(output_dim=64, init='uniform', activation='relu', input_dim=dataIn.shape[1]))\n",
    "# Adding dropout to prevent overfitting\n",
    "model.add(Dropout(p=0.1))\n",
    "# Adding hidden layers\n",
    "model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "# Adding the output layer\n",
    "model.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "# criterion loss and optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "history = model.fit(dataIn, labelsIn,\n",
    "                    batch_size=bSize,\n",
    "                    epochs=nEpochs,\n",
    "                    verbose=False,\n",
    "                    callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model\n",
    "Evaluation based on holdout data. A confusion matrix is printed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Test accuracy is {}%\".format(((cm[0][0] + cm[1][1])/np.sum(cm))*100))\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print('Cohen' + \"\"\"'\"\"\" + 's Kappa = ' + str(kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Plots\n",
    "Create and save plots in work directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Graph Path\n",
    "pwd = os.getcwd()\n",
    "savePathModel = os.path.join(pwd, 'model_fit.png')\n",
    "savePathComBat = os.path.join(pwd, 'combat.png')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['acc'])\n",
    "    # plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='lower right')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    # plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.savefig(savePathModel, dpi=600)\n",
    "\n",
    "# Plot ComBat before & after\n",
    "if harmonize:\n",
    "    szSubPlot = 4                                                                   # Number of features to plot\n",
    "    nBins = 20                                                                      # Number of bins\n",
    "\n",
    "    uniqSites = dFrame.loc[:,'Site'].unique()\n",
    "    with plt.style.context('ggplot'):                                               # Plotting style\n",
    "        fig, axs = plt.subplots(np.sqrt(szSubPlot).astype(int), np.sqrt(szSubPlot).astype(int))\n",
    "        for axsNum, axsIdx in enumerate(axs.reshape(-1)):                                              # Iterate over subplots\n",
    "            plotIdx = random.randint(0,len(dFrame.loc[:,dBegin:dEnd].columns))      # Index random headers\n",
    "            for s in uniqSites:\n",
    "                siteIdx = dFrame.loc[:, 'Site'] == s\n",
    "                nBefore, bBefore = np.histogram(data[siteIdx.values, plotIdx],      # Bin count before\n",
    "                                               bins=nBins,\n",
    "                                               density=True)\n",
    "                nAfter, bAfter = np.histogram(cData[siteIdx.values, plotIdx],       # Bin count after\n",
    "                                             bins=nBins,\n",
    "                                             density=True)\n",
    "\n",
    "                mBefore = np.zeros((nBins,))\n",
    "                mAfter = np.zeros((nBins,))\n",
    "                for i  in range(len(bBefore)-1):                                    # Get median of bin edges\n",
    "                    mBefore[i] = np.median([bBefore[i], bBefore[i + 1]])            # Median of bin edges (before)\n",
    "                    mAfter[i] = np.median([bAfter[i], bAfter[i + 1]])               # Median of bin edges (after)\n",
    "\n",
    "                siteIdx = dFrame.loc[:,'Site'] == s                                 # Extract data for a site\n",
    "                muBefore = np.mean(data[siteIdx.values, plotIdx])\n",
    "                muAfter = np.mean(cData[siteIdx.values, plotIdx])\n",
    "                stdBefore = np.std(data[siteIdx.values, plotIdx])\n",
    "                stdAfter = np.std(cData[siteIdx.values, plotIdx])\n",
    "                yBefore = scipy.stats.norm.pdf(mBefore, muBefore, stdBefore)\n",
    "                yAfter = scipy.stats.norm.pdf(mAfter, muAfter, stdAfter)\n",
    "                if plotType == 'Histogram':\n",
    "                    yBefore = nBefore\n",
    "                    yAfter = nAfter\n",
    "                elif plotType == 'Normal':\n",
    "                    yBefore = scipy.stats.norm.pdf(mBefore, muBefore, stdBefore)\n",
    "                    yAfter = scipy.stats.norm.pdf(mAfter, muAfter, stdAfter)\n",
    "\n",
    "                axsIdx.plot(mBefore, yBefore,                                       # Plot on subplot(axsIdx) before\n",
    "                                  color='#3a4750',\n",
    "                                  alpha=0.25)\n",
    "\n",
    "                axsIdx.plot(mAfter, yAfter,                                         # Plot on subplot(axsIdx) after\n",
    "                                  color='#d72323',\n",
    "                                  alpha=0.25)\n",
    "\n",
    "                if axsNum == 0 or axsNum == 2:\n",
    "                    axsIdx.set_ylabel('% OF SUBJECTS',\n",
    "                                      fontsize=6)\n",
    "\n",
    "                axsIdx.set_xlabel(dFrame.loc[:, dBegin:dEnd].columns[plotIdx].upper(),\n",
    "                                  fontsize=6)\n",
    "\n",
    "        fig.legend(['Before ComBat', 'After ComBat'],                               # Legend\n",
    "                   loc = 'lower right',\n",
    "                   ncol=2,\n",
    "                   fancybox=True,\n",
    "                   bbox_to_anchor=(0.5,-0.1))\n",
    "        plt.suptitle('ComBat Harmonization: Before and After')\n",
    "        plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savePathComBat, dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
