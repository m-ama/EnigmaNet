{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnigmaNet: A Neural Network Framework for Eplipesy Classification\n",
    "### This is a script that uses Keras to predict epilepsy outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Details\n",
    "This script is designed to run with the following configuration:\n",
    "* Keras\n",
    "* TensorFlow backend\n",
    "* PlaidML backend (if using Radeon GPU)\n",
    "* Tabular data (Excel or CSVs)\n",
    "\n",
    "## Dependencies\n",
    "Run the following command to pip install all dependencies\n",
    "```\n",
    "pip install numpy pandas sklearn matplotlib scipy keras_tqdm\n",
    "```\n",
    "\n",
    "Also install NeuroCombat at https://github.com/ncullen93/neuroCombat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to Data\n",
    "Define path to tabular data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = '/Users/sid/Documents/Projects/Enigma-ML/Dataset/all.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Script Parameters\n",
    "Define parameters that control script flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classSel = 'Dx'             # Class labels\n",
    "dBegin = 'AverageFA'              # Column where data begins\n",
    "dEnd = 'ICV'   # Column where data ends\n",
    "cBegin = 'Site'             # Column where covariates/demographics begin\n",
    "cEnd = 'Sex'                # Column where covariates/demographics end\n",
    "fillmissing = True          # Fill missing?\n",
    "harmonize = True            # Run ComBat harmonization?\n",
    "scaleData = True            # Rescale data?\n",
    "dataSplit = 0.10            # Percent of data to remove for validation\n",
    "nEpochs = 1000               # Training number of epochs\n",
    "bSize = 30                  # Training batch size\n",
    "plotType = 'Normal'         # Type of ComBat graphs to save ('Histogram' or 'Normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ComBat Harmonization Parameters\n",
    "Python implementation of ComBat is currently being used, which can be found at https://github.com/ncullen93/neuroCombat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combat Variables\n",
    "if harmonize:\n",
    "    batchVar = 'Site'           # Batch effect variable\n",
    "    discreteVar = ['Dx','Sex']  # Variables which are categorical that you want to predict\n",
    "    continuousVar = ['Age']     # Variables which are continuous that you want to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules\n",
    "Import all modules required to process the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuroCombat import neuroCombat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent Function\n",
    "Functions that the scrips will depend on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classfill(dFrame, classCol, siteCol, idxRange):\n",
    "    \"\"\"Fills missing values with means of a class\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    dFrame:   Pandas dataframe to process (type: dataframe)\n",
    "            \n",
    "    classCol: String indicating dataframe column name containing class information\n",
    "\n",
    "    siteCol:  String indicating dataframc column name containing class information\n",
    "    \n",
    "    idxRange: 2x1 vector indicating lower and upper bound of data to fill in dataframe\n",
    "              idxRange[0] is lower bound\n",
    "              idxRange[1] is upper bound\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data:     Dataframe will all missing values filled\n",
    "    \"\"\"\n",
    "    uniqClass = dFrame[classCol].unique()                   # All unique classes\n",
    "    uniqSites = dFrame[siteCol].unique()                    # All unique sites\n",
    "    print('...found ' + str(uniqClass.size) + ' classes across ' + str(uniqSite.size) + ' sites')\n",
    "    print('...filling missing data with class means')\n",
    "    data = dFrame.loc[:, idxRange[0]:idxRange[1]]           # Extract all numerical value from 'dBegin' onwards\n",
    "    for site in uniqSites:\n",
    "        siteIdx = dFrame.loc[:, siteCol] == site            # Index where site is uniqSite = site\n",
    "        for cls in uniqClass:\n",
    "            classIdx = dFrame.loc[:, classCol] == cls       # Index where class is uniqClass = cls\n",
    "            idx = np.multiply(siteIdx, classIdx)            # Index where both class and site indexes are true\n",
    "            for col in range(len(data.columns)):            # Iterate along each column\n",
    "                nanIdx = data.iloc[: ,col].isnull()         # Index where NaNs occur per feature\n",
    "                nanIdx_i = np.multiply(nanIdx, idx)         # Index where NaNs occur per feauture, per site, per class\n",
    "                if np.sum(nanIdx_i) >= 1:\n",
    "                    mean = np.nanmean(data.iloc[:, col][idx]) # Compute mean of non-NaNs# If there are any Nans...\n",
    "                    data.iloc[:, col][nanIdx_i] = mean      # Replace NaNs with mean\n",
    "    dFrame.loc[:, idxRange[0]:idxRange[1]] = data           # Substitute dataframe with corrected data\n",
    "    return dFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files and Manipulate\n",
    "Load tabular data and prime it for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...found 2 classes\n",
      "...filling missing data with class means\n",
      "Creating design matrix..\n",
      "Standardizing data across features..\n",
      "Fitting L/S model and finding priors..\n",
      "Finding parametric adjustments..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3367: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/_methods.py:130: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/sid/miniconda3/envs/deep/lib/python3.7/site-packages/neuroCombat/neuroCombat.py:180: RuntimeWarning: Mean of empty slice.\n",
      "  m = gamma_hat.mean()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9fdf455c33f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                           \u001b[0mbatch_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchVar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                           \u001b[0mdiscrete_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscreteVar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                           continuous_cols=continuousVar)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdBegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdEnd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Preserve non-harmonized data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/neuroCombat/neuroCombat.py\u001b[0m in \u001b[0;36mneuroCombat\u001b[0;34m(data, covars, batch_col, discrete_cols, continuous_cols)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# find parametric adjustments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finding parametric adjustments..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mgamma_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_parametric_adjustments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLS_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# adjust data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/neuroCombat/neuroCombat.py\u001b[0m in \u001b[0;36mfind_parametric_adjustments\u001b[0;34m(s_data, LS, info_dict)\u001b[0m\n\u001b[1;32m    240\u001b[0m         temp = it_sol(s_data[:,batch_idxs], LS['gamma_hat'][i],\n\u001b[1;32m    241\u001b[0m                     \u001b[0mLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma_bar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                     LS['a_prior'][i], LS['b_prior'][i])\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mgamma_star\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/neuroCombat/neuroCombat.py\u001b[0m in \u001b[0;36mit_sol\u001b[0;34m(sdat, g_hat, d_hat, g_bar, t2, a, b, conv)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0md_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_new\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mg_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mg_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_new\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mg_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;31m#.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0md_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_new\u001b[0m \u001b[0;31m#.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m     26\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     27\u001b[0m           initial=_NoValue):\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "dFrame = pd.read_csv(csvPath)           # Dataframe\n",
    "if fillmissing:\n",
    "    dFrame = classfill(dFrame, classSel, [dBegin, dEnd])\n",
    "else:\n",
    "    print('...skip fill missing')\n",
    "\n",
    "# Run combat\n",
    "if harmonize:\n",
    "    cData = neuroCombat(data=dFrame.loc[:,dBegin:dEnd],\n",
    "                          covars=dFrame.loc[:,cBegin:cEnd],\n",
    "                          batch_col=batchVar,\n",
    "                          discrete_cols=discreteVar,\n",
    "                          continuous_cols=continuousVar)\n",
    "\n",
    "data = np.array(dFrame.loc[:, dBegin:dEnd])     # Preserve non-harmonized data\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()   # Initialize scaler\n",
    "if scaleData:\n",
    "    if harmonize:\n",
    "        cData = scaler.fit_transform(cData)\n",
    "    data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data for Training and Evaluation\n",
    "Data needs to be split for trianing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets and scale\n",
    "if harmonize:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cData, dFrame.loc[:, classSel], test_size=dataSplit, random_state=0)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, dFrame.loc[:, classSel], test_size=dataSplit, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Train\n",
    "A perceptron is constructed here for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose whether data/labels or X_Train,y_train\n",
    "dataIn = X_train\n",
    "labelsIn = y_train\n",
    "\n",
    "# Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the Single Perceptron or Shallow network\n",
    "model.add(Dense(output_dim=64, init='uniform', activation='relu', input_dim=dataIn.shape[1]))\n",
    "# Adding dropout to prevent overfitting\n",
    "model.add(Dropout(p=0.1))\n",
    "# Adding hidden layers\n",
    "#model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "# Adding the output layer\n",
    "model.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "# criterion loss and optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "history = model.fit(dataIn, labelsIn,\n",
    "                    batch_size=bSize,\n",
    "                    epochs=nEpochs,\n",
    "                    verbose=False,\n",
    "                    callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model\n",
    "Evaluation based on holdout data. A confusion matrix is printed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Test accuracy is {}%\".format(((cm[0][0] + cm[1][1])/np.sum(cm))*100))\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print('Cohen' + \"\"\"'\"\"\" + 's Kappa = ' + str(kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Plots\n",
    "Create and save plots in work directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Graph Path\n",
    "pwd = os.getcwd()\n",
    "savePathModel = os.path.join(pwd, 'model_fit.png')\n",
    "savePathComBat = os.path.join(pwd, 'combat.png')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['acc'])\n",
    "    # plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='lower right')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    # plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.savefig(savePathModel, dpi=600)\n",
    "\n",
    "# Plot ComBat before & after\n",
    "if harmonize:\n",
    "    szSubPlot = 4                                                                   # Number of features to plot\n",
    "    nBins = 20                                                                      # Number of bins\n",
    "\n",
    "    uniqSites = dFrame.loc[:,'Site'].unique()\n",
    "    with plt.style.context('ggplot'):                                               # Plotting style\n",
    "        fig, axs = plt.subplots(np.sqrt(szSubPlot).astype(int), np.sqrt(szSubPlot).astype(int))\n",
    "        for axsNum, axsIdx in enumerate(axs.reshape(-1)):                                              # Iterate over subplots\n",
    "            plotIdx = random.randint(0,len(dFrame.loc[:,dBegin:dEnd].columns))      # Index random headers\n",
    "            for s in uniqSites:\n",
    "                siteIdx = dFrame.loc[:, 'Site'] == s\n",
    "                nBefore, bBefore = np.histogram(data[siteIdx.values, plotIdx],      # Bin count before\n",
    "                                               bins=nBins,\n",
    "                                               density=True)\n",
    "                nAfter, bAfter = np.histogram(cData[siteIdx.values, plotIdx],       # Bin count after\n",
    "                                             bins=nBins,\n",
    "                                             density=True)\n",
    "\n",
    "                mBefore = np.zeros((nBins,))\n",
    "                mAfter = np.zeros((nBins,))\n",
    "                for i  in range(len(bBefore)-1):                                    # Get median of bin edges\n",
    "                    mBefore[i] = np.median([bBefore[i], bBefore[i + 1]])            # Median of bin edges (before)\n",
    "                    mAfter[i] = np.median([bAfter[i], bAfter[i + 1]])               # Median of bin edges (after)\n",
    "\n",
    "                siteIdx = dFrame.loc[:,'Site'] == s                                 # Extract data for a site\n",
    "                muBefore = np.mean(data[siteIdx.values, plotIdx])\n",
    "                muAfter = np.mean(cData[siteIdx.values, plotIdx])\n",
    "                stdBefore = np.std(data[siteIdx.values, plotIdx])\n",
    "                stdAfter = np.std(cData[siteIdx.values, plotIdx])\n",
    "                yBefore = scipy.stats.norm.pdf(mBefore, muBefore, stdBefore)\n",
    "                yAfter = scipy.stats.norm.pdf(mAfter, muAfter, stdAfter)\n",
    "                if plotType == 'Histogram':\n",
    "                    yBefore = nBefore\n",
    "                    yAfter = nAfter\n",
    "                elif plotType == 'Normal':\n",
    "                    yBefore = scipy.stats.norm.pdf(mBefore, muBefore, stdBefore)\n",
    "                    yAfter = scipy.stats.norm.pdf(mAfter, muAfter, stdAfter)\n",
    "\n",
    "                axsIdx.plot(mBefore, yBefore,                                       # Plot on subplot(axsIdx) before\n",
    "                                  color='#3a4750',\n",
    "                                  alpha=0.25)\n",
    "\n",
    "                axsIdx.plot(mAfter, yAfter,                                         # Plot on subplot(axsIdx) after\n",
    "                                  color='#d72323',\n",
    "                                  alpha=0.25)\n",
    "\n",
    "                if axsNum == 0 or axsNum == 2:\n",
    "                    axsIdx.set_ylabel('NORMALIZED SUBJECTS',\n",
    "                                      fontsize=6)\n",
    "\n",
    "                axsIdx.set_xlabel(dFrame.loc[:, dBegin:dEnd].columns[plotIdx].upper(),\n",
    "                                  fontsize=6)\n",
    "\n",
    "        fig.legend(['Before ComBat', 'After ComBat'],                               # Legend\n",
    "                   loc = 'lower right',\n",
    "                   ncol=2,\n",
    "                   fancybox=True,\n",
    "                   bbox_to_anchor=(0.5,-0.1))\n",
    "        plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savePathComBat, dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
